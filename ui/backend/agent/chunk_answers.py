from .imports import * 
from .llm import get_llm
llm = get_llm()



def _chunk_student_answers_parallel(student_answer:str,
                                    marking_scheme:List[Tuple[str,Union[int,float]]],
                                    )->List[str]:
    
    
    results = [_chunk_student_answers(student_answer,marking_scheme[i][0]) for i in range(len(marking_scheme))]
    # with Pool(processes=max_jobs_at_a_time) as pool:
    #     results = pool.starmap(_chunk_student_answers,inputs)
    
    
    for i in range(len(results)):
        print("------------------------------------")        
        print(results[i])
        print("------------------------------------")        
    print("\n")
    return results


def _chunk_student_answers(
                            student_answer:str,
                            marking_scheme:str,
                        )->str:
    
    
    '''
    This function takes up inputs as student answers and marking scheme. Based on that,
    it returns a string which is a single chunk that are to be checked by the marking scheme.
    '''
    base_prompt = ChatPromptTemplate(messages = [

    SystemMessage('''You are a very experienced teacher. You are given an instruction of a marking scheme which are used to grade the student's answer. You're provided with the answer of the student . You're supposed to identify and return that exact section that is to be evaluated.     Adhere to the following points : 
    1) The text should be strictly from the student's answer. 
    2) The text should not be modified in any case. 
    3) Don't include any text generated by you.
    4) Include a starting tag <start-of-answer> for marking the beginning of the answer chunk. Similarly an ending tag <end-of-answer> ending of answer chunk. 
    5) If no relevant piece of text is found, return the text : <start-of-answer><empty response><end-of-answer>.'''),
    ("human","MARKING SCHEME : {marking_scheme}\nSTUDENT ANSWER : {student_answer}")
                                                ])
    inputs_to_llm = {"student_answer":student_answer,"marking_scheme":marking_scheme} 
    chain = base_prompt | llm | StrOutputParser()
    result = chain.invoke(inputs_to_llm) 
    # Extracting the text
    index= result.find("</think>")
    final_answer = result[index+8:].strip()
    
    return final_answer if final_answer.__contains__("<empty response>")==False else ""